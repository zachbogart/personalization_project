{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "Zach Bogart, Josh Feldman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this homework, we are trying to create movie recommendations for users based on the MovieLens dataset. We want to create recommendations that are relevant to the users, giving new movies thay have not seen, but might enjoy since other users with similar watch patterns have rated them highly.\n",
    "- We explored two models of collaborative filtering: one model-based and one neighborhood-based. We used non-negative matrix factorization (model-based) and k-nearest neighbors (neighborhood-based). We are interested in seeing how the models differ in accuracy metrics and what sacrifices may have to be made to achieve those results.\n",
    "- For our accuracy metrics, we used root mean squared error and mean absolute error. We used both of these metrics because they weighted outliers differently, giving a different sense of how accurate our results are. RMSE weights outliers more heavily. We chose RMSE as our primary accuracy metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Process (the python script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To test the ability of the model to give good predictions to users, we needed to get the best parameters for the model. To do this, we did cross validation using gridSearchCV to test the effect different hyperparameters had on the model. We split the cross validation into separate grid searches since the models take a long time to run (due to the size of the dataset). \n",
    "- After grid search, we save off the best parameters.\n",
    "- With the best parameters, we systematically tested how the size of the training data affected results, collecting accuracy metrics and runtime figures.\n",
    "- Finally, we used the best hyperparameters to train the models and evaluate them on the test data. We then saved off these models to be used to make recommendations.\n",
    "\n",
    "- With this, we have a setup where we can pass in a model, cross-validate the desired hyperparameters, and report accuracy metrics based on those choices to find the best setup. Below we will show the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **NOTE**: This notebook expects the results of the script. We saved off the results to be used. However, you can run the script if you want (it takes a wahile on the large dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Script Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultFiles = {}\n",
    "\n",
    "for filename in os.listdir('results'):\n",
    "    if \".sav\" not in filename:\n",
    "        infile = open(\"results/{}\".format(filename), 'rb')\n",
    "        result = pickle.load(infile)\n",
    "        resultFiles[filename] = result\n",
    "        infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our resultsFiles contains 3 separate results:\n",
    "- Files starting with 'gridSearchResults' are results from parameter tuning. We saved not only the best parameters and metrics, but also the entire cv_results from the gridSearch\n",
    "- Files starting with 'dataSizeResults' are lists of results from running our models with different data sizes\n",
    "- Files starting with 'finalResults' are results from the final run of our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the NMF setup, there are a set of parameters we can tune:\n",
    "\n",
    "- n_epochs: number of iterations of stochastic gradient descent\n",
    "- n_factors: number of intermediate factors\n",
    "- reg_pu, reg_qi: regularization coefficients\n",
    "- biased: whether to use baselines\n",
    "\n",
    "Let's see how they affect the accuracy metrics. We have created a function to create the plots of each hyperparamater for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plotParameterResults(gridSearchFiles, modelType):\n",
    "    bestParams = {}\n",
    "    for gridSearchKey in gridSearchFiles.keys():\n",
    "        if \"gridSearchResults{}\".format(modelType) in gridSearchKey:\n",
    "            periodIndex = gridSearchKey.find('.')\n",
    "            underscoreIndex = gridSearchKey.find('_')\n",
    "            param = gridSearchKey[underscoreIndex + 1:periodIndex]\n",
    "        \n",
    "            if param == 'random_state' or param == 'verbose':\n",
    "                continue\n",
    "\n",
    "            gridSearch = gridSearchFiles[gridSearchKey]\n",
    "            \n",
    "            bestParamValue = gridSearch['bestParams'][param]\n",
    "            bestParams[param] = bestParamValue\n",
    "\n",
    "            for index, resultType in enumerate(['mean_test_mae', 'mean_test_rmse']):\n",
    "                x = [str(result) for result in gridSearch[\"cv_results\"]['param_{}'.format(param)]]\n",
    "                y = [result for result in gridSearch[\"cv_results\"][resultType]]\n",
    "\n",
    "                plt.bar(x, y, color='black')\n",
    "                plt.title('Matrix Factorization Model, {}'.format(param))\n",
    "                plt.xlabel(param)\n",
    "                plt.ylabel(resultType)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "    return bestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bestParamsNMF = plotParameterResults(resultFiles, 'NMF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The best params using rmse are:')\n",
    "for key, value in bestParamsNMF.items():\n",
    "    print('{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhood Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestParamsKNN = plotParameterResults(resultFiles, 'KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The best params using rmse are:')\n",
    "for key, value in bestParamsKNN.items():\n",
    "    print('{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def plotSampleSizes(gridSearchFiles):\n",
    "filename = \"dataSizeResultsKNN.p\"\n",
    "resultList = resultFiles[filename]\n",
    "timeKNN = [item['time'] for item in resultList]\n",
    "rmseKNN = [item['rmse'] for item in resultList]\n",
    "numRatingsKNN = [item['numRatings'] for item in resultList]\n",
    "\n",
    "filename = \"dataSizeResultsNMF.p\"\n",
    "resultList = resultFiles[filename]\n",
    "timeNMF = [item['time'] for item in resultList]\n",
    "rmseNMF = [item['rmse'] for item in resultList]\n",
    "numRatingsNMF = [item['numRatings'] for item in resultList]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(numRatingsNMF, rmseNMF, 'o', color='black')\n",
    "plt.title('Matrix Factorization Model')\n",
    "plt.xlabel('Data Sample Percentage')\n",
    "plt.ylabel('rmse')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(numRatingsNMF, timeNMF, 'o', color='black')\n",
    "plt.title('Matrix Factorization Model')\n",
    "plt.xlabel('Data Sample Percentage')\n",
    "plt.ylabel('Runtime to Train')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(numRatingsKNN, rmseKNN, 'o', color='black')\n",
    "plt.title('Matrix Factorization Model')\n",
    "plt.xlabel('Data Sample Percentage')\n",
    "plt.ylabel('rmse')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(numRatingsKNN, timeKNN, 'o', color='black')\n",
    "plt.title('Matrix Factorization Model')\n",
    "plt.xlabel('Data Sample Percentage')\n",
    "plt.ylabel('Runtime to Train')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**:\n",
    "\n",
    "- For NMF, as you add more data, the RMSE exponentially decreases. Meanwhile, the runtime increases linearly as you add more data. This makes sense since more data will of course take longer to train. However, we can see that one of our choices in implimenting this model. At a certain point, adding data no longer changes the accuracy metric by much (for example, doubling the data from 10K to 20K makes a noticable difference, while doubling from 30K to 60K makes much less of a difference). This should be taken into account when deciding how much data to use to train the model.\n",
    "\n",
    "- If the goal is to make the model quick to train, you could sacrifice some accuracy for a large gain in runtime.\n",
    "\n",
    "- A similar result is seen in the knn model. Again, it should be considered when deploying whether to sacrifice a small amount of accuracy for a gain in runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF vs. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultsNMF = resultFiles['finalResultsNMF.p']\n",
    "resultsKNN = resultFiles['finalResultsKNN.p']\n",
    "models = ['NMF', 'KNN']\n",
    "rmseScores = resultsNMF['rsme'], resultsKNN['rsme']\n",
    "maeScores = resultsNMF['mae'], resultsKNN['mae']\n",
    "\n",
    "plt.bar(models, rmseScores, color='black')\n",
    "plt.title('Matrix Factorization vs KNN')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('rmse')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.bar(models, maeScores, color='black')\n",
    "plt.title('Matrix Factorization vs KNN')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('mae')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, from our final results we can see that our KNN model seems to outperform our Matrix Factorization model, both in our accuracy metrics and in the time to train. So, for deploying this model, we recommend (pun intended) using the knn model since it is much faster to train and gives better accuracy for recommendations related to the users. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
